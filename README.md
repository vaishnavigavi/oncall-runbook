# OnCall Runbook - AI-Powered Incident Response

A comprehensive monorepo for managing and querying oncall runbooks and incident response documentation using AI-powered search and retrieval.

## üöÄ Quick Start

```bash
# Clone and setup
git clone <repository-url>
cd oncall-runbook

# Start all services
make dev

# Or start individually
make api      # Start API only
make web      # Start web only
```

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ api/                 # FastAPI backend service
‚îú‚îÄ‚îÄ web/                 # React + Vite frontend
‚îú‚îÄ‚îÄ data/                # Data directories
‚îÇ   ‚îú‚îÄ‚îÄ docs/           # Documentation files
‚îÇ   ‚îú‚îÄ‚îÄ index/          # FAISS index storage
‚îÇ   ‚îî‚îÄ‚îÄ mock/           # Mock logs and test data
‚îú‚îÄ‚îÄ docker/              # Docker configuration files
‚îú‚îÄ‚îÄ .env.example         # Environment variables template
‚îú‚îÄ‚îÄ docker-compose.yml   # Multi-service orchestration
‚îú‚îÄ‚îÄ Makefile            # Development commands
‚îî‚îÄ‚îÄ requirements.txt     # Python dependencies
```

## üõ†Ô∏è Development Commands

```bash
make dev          # Start all services
make web          # Start web frontend only
make api          # Start API only
make ingest       # Run document ingestion
make selfcheck    # Verify services and data
make build        # Build Docker images
make clean        # Cleanup Docker containers
make logs         # View all service logs
```

## üåê API Endpoints

### Core Endpoints
- `GET /health` - Health check
- `POST /ingest` - Document ingestion
- `POST /ask` - Question answering (legacy format)
- `POST /ask/structured` - Structured RAG responses
- `GET /stats` - FAISS index statistics

### Document Ingestion
```bash
# Ingest seed documents
curl -X POST "http://localhost:8000/ingest" \
  -H "Content-Type: application/json" \
  -d '{"ingest_seed_docs": true}'

# Ingest specific file
curl -X POST "http://localhost:8000/ingest" \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/app/data/docs/alerts-cheatsheet.md"}'
```

### Question Answering
```bash
# Ask a question with structured response
curl -X POST "http://localhost:8000/ask/structured" \
  -H "Content-Type: application/json" \
  -d '{"question": "High CPU > 85% for 5 min after deploy", "context": ""}'
```

## üí¨ Frontend Features

### Chat Interface
- **Single page chat**: Clean, modern chat interface
- **Input & Send**: Type questions and send with button
- **Messages list**: View conversation history
- **Loading states**: Visual feedback during processing

### Sources & Citations
- **Sources chips**: Clickable citation chips below answers
- **Side panel**: Click chips to open source details
- **Chunk content**: View actual document text for each citation
- **Citation format**: `filename#chunk_id` (e.g., `alerts-cheatsheet.md#e195f2a3`)

### Error Handling
- **Graceful errors**: Toast notifications for user feedback
- **API fallbacks**: Handles connection issues gracefully
- **Loading states**: Clear visual feedback

### Environment Configuration
```bash
# Frontend environment variables
VITE_API_BASE=http://localhost:8000  # API base URL
```

## üîç RAG System Features

### Document Processing
- **Multiple formats**: `.md`, `.txt`, `.pdf` support
- **Smart chunking**: Heading-aware with 800-token chunks
- **Overlap handling**: 100-token overlap for context
- **PDF support**: PyMuPDF with PyPDF2 fallback

### Embedding & Search
- **OpenAI integration**: Azure OpenAI and OpenAI support
- **Mock fallback**: Works without API keys
- **FAISS index**: Efficient similarity search
- **Keyword fallback**: Intelligent content matching

### Answer Structure
```
**First checks:**
‚Ä¢ Check for runaway processes
‚Ä¢ Review recent deployments

**Why this happens:**
‚Ä¢ Infinite loops in application code
‚Ä¢ Database query issues

**Diagnostics if tools ran:**
‚Ä¢ `top` or `htop` to see process list
‚Ä¢ `free -h` to see memory status

**Sources:** See citations below.
```

## üìä Data & Storage

### Seed Documents
- **Payments Runbook**: Payment system troubleshooting
- **Incident Response**: P0/P1 incident procedures
- **Alerts Cheat Sheet**: CPU, memory, disk alerts
- **SLO Policy**: Service level objectives

### FAISS Index
- **Persistent storage**: `/app/data/index/`
- **Metadata tracking**: Chunk information and sources
- **Upsert support**: Add documents without rebuilding
- **Statistics**: Real-time index metrics

## üê≥ Docker Configuration

### Services
- **API**: FastAPI with Python 3.11
- **Web**: React + Vite + Tailwind
- **Networking**: Isolated network with health checks
- **Volumes**: Persistent data storage

### Health Checks
- **API**: `/health` endpoint monitoring
- **Web**: HTTP response validation
- **Dependencies**: Service startup ordering

## üß™ Testing & Validation

### Acceptance Criteria
- ‚úÖ **Docker compose up --build** ‚Üí Services start successfully
- ‚úÖ **GET /health** ‚Üí Returns "ok" status
- ‚úÖ **Web interface** ‚Üí Opens and displays chat
- ‚úÖ **Seed documents** ‚Üí Present on disk
- ‚úÖ **POST /ingest** ‚Üí Ingests documents successfully
- ‚úÖ **FAISS files** ‚Üí Index and metadata created
- ‚úÖ **Re-calling /ingest** ‚Üí Idempotent operation
- ‚úÖ **POST /ask** ‚Üí Returns structured answers with citations
- ‚úÖ **Frontend chat** ‚Üí Ask questions and see cited answers
- ‚úÖ **Source chips** ‚Üí Click to open source text panel

### Manual Testing
```bash
# Test ingestion
make ingest

# Test question answering
curl -X POST "http://localhost:8000/ask/structured" \
  -H "Content-Type: application/json" \
  -d '{"question": "High CPU usage alert", "context": ""}'

# Test self-check
make selfcheck
```

## üîß Configuration

### Environment Variables
```bash
# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true

# Web Configuration
WEB_PORT=3000
WEB_HOST=0.0.0.0

# OpenAI Configuration
OPENAI_API_KEY=your_key_here
OPENAI_MODEL=gpt-3.5-turbo
OPENAI_EMBEDDING_MODEL=text-embedding-ada-002

# Azure OpenAI (alternative)
AZURE_OPENAI_API_KEY=your_key_here
AZURE_OPENAI_ENDPOINT=your_endpoint_here

# Document Processing
CHUNK_SIZE=800
CHUNK_OVERLAP=100
FAISS_DIMENSION=1536
```

## üöÄ Production Deployment

### Requirements
- Docker and Docker Compose
- OpenAI API key or Azure OpenAI credentials
- Sufficient storage for document index
- Network access for API calls

### Scaling
- **API**: Multiple workers with load balancer
- **Web**: Static build with CDN
- **Index**: Shared storage for FAISS files
- **Monitoring**: Health checks and logging

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

### Development Setup
```bash
# Install dependencies
cd api && pip install -r requirements.txt
cd ../web && npm install

# Run locally
cd ../api && python main.py
cd ../web && npm run dev
```

## üìù License

This project is licensed under the MIT License - see the LICENSE file for details.

## üÜò Support

For issues and questions:
- Check the documentation
- Review the logs: `make logs`
- Run self-check: `make selfcheck`
- Open an issue on GitHub
